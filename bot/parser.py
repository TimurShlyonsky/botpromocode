import requests
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

BASE_URL = "https://www.lotro.com"
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept-Language": "en-US,en;q=0.9",
}

# ‚ùó –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –≤—ã–ª–∞–≤–ª–∏–≤–∞–Ω–∏—è JSON —Å–æ —Å—Ç–∞—Ç—å—è–º–∏
ARCHIVE_JSON_RE = re.compile(
    r"window\.SSG\.archive\.articles\s*=\s*(\[.*?\]);",
    re.S
)


def get_month_news(year: int, month: int) -> list[dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤ –º–µ—Å—è—Ü–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π —Å—Ç—Ä–æ–≥–æ –∏–∑ —ç—Ç–æ–≥–æ –º–µ—Å—è—Ü–∞:
    [
        {"url": "...", "title": "..."}
    ]
    """
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"üìÇ –ê—Ä—Ö–∏–≤: {url}")

    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load archive: {e}")
        return []

    match = ARCHIVE_JSON_RE.search(resp.text)
    if not match:
        print("‚ö†Ô∏è No JSON found in archive page!")
        return []

    articles = json.loads(match.group(1))
    print(f"üîó –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –≤ JSON: {len(articles)}")

    filtered = []
    for a in articles:
        date_str = a.get("publishDate")
        if not date_str:
            continue

        # –ù–∞–ø—Ä–∏–º–µ—Ä: "2025-12-04T12:00:00.000Z"
        try:
            dt = datetime.strptime(date_str[:10], "%Y-%m-%d")
        except:
            continue

        # ‚ö†Ô∏è –§–ò–õ–¨–¢–†–£–ï–ú –°–¢–†–û–ì–û –ü–û –ú–ï–°–Ø–¶–£
        if dt.year == year and dt.month == month:
            page = a.get("pageName")
            if page:
                filtered.append({
                    "url": f"{BASE_URL}/news/{page}",
                    "title": a.get("title", "No title")
                })

    print(f"üéØ –°—Ç–∞—Ç–µ–π –∑–∞ –º–µ—Å—è—Ü: {len(filtered)}")
    return filtered


PROMO_RE = re.compile(r"(?:coupon code|use code|use coupon code)[:\s]+([A-Z0-9]+)",
                      re.IGNORECASE)


def extract_promo_from_news(url: str) -> list[dict]:
    """–ò—â–µ–º –ø—Ä–æ–º–æ–∫–æ–¥—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç–∞—Ç—å–∏"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load page: {url} | {e}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    body = soup.select_one(".article-body")
    if not body:
        return []

    text = body.get_text(" ", strip=True)

    found = []
    for code in set(PROMO_RE.findall(text)):
        # –§–∏–ª—å—Ç—Ä –Ω–µ–Ω—É–∂–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–∫–æ—Ä–æ—Ç–∫–∏—Ö, –æ–±—â–∏—Ö —Å–ª–æ–≤)
        if len(code) < 5:
            continue

        found.append({
            "code": code.upper(),
            "url": url
        })

    return found

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import re

BASE_URL = "https://www.lotro.com"

# –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –∫–æ–¥—ã, –Ω–æ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∏–º–∏
BLACKLIST = {"CODE", "FREE", "HAS", "IS", "OF", "FOR", "CAN", "WILL", "THROUGH"}


def is_valid_code(code: str) -> bool:
    """–§–∏–ª—å—Ç—Ä—É–µ–º –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è."""
    if len(code) < 6:
        return False
    if code in BLACKLIST:
        return False
    if not re.match(r"^[A-Z0-9]+$", code):
        return False
    return True


def get_month_news(year: int, month: int) -> list[str]:
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"üìÇ –ê—Ä—Ö–∏–≤: {url}")

    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
    except Exception:
        return []

    text = response.text

    # JSON news list
    match = re.search(r"window\.SSG\.archive\.articles\s*=\s*(\[[\s\S]*?\]);",
                      text)
    if not match:
        print("‚ö†Ô∏è JSON –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []

    try:
        entries = json.loads(match.group(1))
    except Exception:
        print("‚ö†Ô∏è JSON parse error")
        return []

    links = set()
    for e in entries:
        href = e.get("url") or e.get("pageName")
        if not href:
            continue
        if not href.startswith("/"):
            href = "/" + href
        if "/news/" not in href:
            href = "/news" + href
        links.add(urljoin(BASE_URL, href))

    print(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
    return sorted(links)


def extract_promo_from_news(url: str):
    """–ò—â–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–æ–∫–æ–¥—ã –≤ —Ç–µ–ª–µ —Å—Ç–∞—Ç—å–∏."""
    try:
        res = requests.get(url, timeout=20)
        res.raise_for_status()
    except Exception:
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–∞–º–æ–π —Å—Ç–∞—Ç—å–∏
    body = soup.select_one(".article-body")
    if not body:
        return []

    text = body.get_text(" ", strip=True)
    text_u = text.upper()

    promos = []

    # –í—ã–¥—ë—Ä–≥–∏–≤–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç "Coupon Code: XXX"
    for m in re.finditer(r"(COUPON CODE|USE CODE|PROMO CODE)[:\s]+([A-Z0-9]+)", text_u):
        code = m.group(2).strip().upper()

        if not is_valid_code(code):
            continue

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä—è–¥–æ–º (–¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤)
        desc = extract_description(text, code)

        promos.append({
            "code": code,
            "description": desc,
            "url": url
        })

    return promos


def extract_description(full_text: str, code: str) -> str | None:
    """–û–ø–∏—Å–∞–Ω–∏–µ ‚Äî 100 —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –∏ –ø–æ—Å–ª–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞."""
    pos = full_text.upper().find(code)
    if pos == -1:
        return None

    start = max(0, pos - 100)
    end = min(len(full_text), pos + len(code) + 100)
    snippet = full_text[start:end]

    # –ú–∏–Ω–∏-—Ñ–∏–ª—å—Ç—Ä –æ–ø–∏—Å–∞–Ω–∏—è
    if len(snippet) < 20:
        return None

    if any(key in snippet.upper() for key in ["FREE", "%", "BOOST", "XP"]):
        return " ".join(snippet.split())

    return None


if __name__ == "__main__":
    for url in get_month_news(2025, 12):
        print(extract_promo_from_news(url))

import re
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

BASE_URL = "https://www.lotro.com"

HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept-Language": "en-US,en;q=0.9",
}


def get_month_news(year: int, month: int) -> list[str]:
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ°Ñ€Ñ…Ğ¸Ğ²Ğ° Ğ¼ĞµÑÑÑ†Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑÑ‹Ğ»ĞºĞ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸,
    Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ½ÑƒĞ¶Ğ½Ğ¾Ğ¼ Ğ¼ĞµÑÑÑ†Ğµ Ğ¸ Ğ³Ğ¾Ğ´Ñƒ.
    """
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"ğŸ“‚ ĞÑ€Ñ…Ğ¸Ğ²: {url}")

    try:
        res = requests.get(url, timeout=20, headers=HEADERS)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ Archive fetch failed: {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    articles = soup.select("article.archive-item")
    print(f"ğŸ” ĞĞ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ ÑÑ‚Ğ°Ñ‚ĞµĞ¹: {len(articles)}")

    links = []

    for art in articles:
        date_el = art.select_one(".metadata__date")
        a_tag = art.select_one("a[href]")
        if not a_tag:
            continue

        href = a_tag["href"]
        full_url = urljoin(BASE_URL, href)

        # Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ñ‚Ñ‹ Ğ½ĞµÑ‚ â€” Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼
        if not date_el:
            continue

        date_text = date_el.get_text(strip=True)

        # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°: "Dec 4th, 2025"
        try:
            dt = datetime.strptime(date_text, "%b %dth, %Y")
        except:
            continue

        if dt.year == year and dt.month == month:
            links.append(full_url)

    print(f"ğŸ¯ Ğ¡ÑÑ‹Ğ»Ğ¾Ğº Ğ·Ğ° Ğ¼ĞµÑÑÑ†: {len(links)}")
    return sorted(set(links))


def extract_promo_from_news(url: str) -> list[dict]:
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ñ‰ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ĞºĞ¾Ğ´Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ.
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²: {"code", "title", "url"}
    """
    try:
        res = requests.get(url, timeout=20, headers=HEADERS)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ News fetch failed {url}: {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    title = soup.select_one("h1")
    title_text = title.get_text(strip=True) if title else "Promo"

    body = soup.select_one(".article-body")
    if not body:
        return []

    text = body.get_text(" ", strip=True)
    text_upper = text.upper()

    # ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½: COUPON CODE: XXXXXXX
    matches = re.findall(r"COUPON CODE[:\s]+([A-Z0-9]+)", text_upper)

    results = []
    for code in set(matches):
        if len(code) >= 6:
            results.append({
                "code": code,
                "title": title_text,
                "url": url
            })

    return results

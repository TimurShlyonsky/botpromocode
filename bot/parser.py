import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE_URL = "https://www.lotro.com"


def get_month_news(year: int, month: int) -> list[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏.
    """
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"üîé Fetching archive: {url}")

    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load archive page: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")

    # –í –∞—Ä—Ö–∏–≤–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–¥–≥—Ä—É–∂–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ JS
    # –ù–æ —Å—Å—ã–ª–∫–∏ –µ—Å—Ç—å –≤ HTML –≤–Ω—É—Ç—Ä–∏ —Ç–µ–º–ø–ª–µ–π—Ç–∞ ‚Üí –∏—â–µ–º –≤—Å–µ <a href>
    links = set()
    for a in soup.find_all("a", href=True):
        href = a["href"]
        # –ò–Ω—Ç–µ—Ä–µ—Å—É—é—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏
        if href.startswith("/news/") or href.startswith("/update-notes/") or href.startswith("/guides/"):
            full_url = urljoin(BASE_URL, href)
            links.add(full_url)

    return sorted(list(links))


if __name__ == "__main__":
    # –¢–µ—Å—Ç: –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –¥–µ–∫–∞–±—Ä—å 2025
    urls = get_month_news(2025, 12)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Ç–∞—Ç–µ–π:")
    for u in urls:
        print(" -", u)
import re


def extract_promo_from_news(url: str):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∏ –∏—â–µ—Ç –ø—Ä–æ–º–æ–∫–æ–¥—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤: {"code", "description", "url"}
    """
    try:
        res = requests.get(url, timeout=20)
        res.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load news page: {url} | {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ç–µ–∫—Å—Ç—ã –∞–±–∑–∞—Ü–µ–≤
    paragraphs = soup.find_all(["p", "div", "span", "li"])

    # –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–∞–ø–∏—Å–∞–Ω–∏—è
    patterns = [
        r"Coupon Code[: ]+([A-Z0-9]+)",
        r"Use Code[: ]+([A-Z0-9]+)",
        r"Use coupon code[: ]+([A-Z0-9]+)",
        r"Code[: ]+([A-Z0-9]+)",
        r"Coupon[: ]+([A-Z0-9]+)",
    ]

    found = []

    for p in paragraphs:
        text = " ".join(p.get_text(" ", strip=True).split())
        if not text:
            continue

        for pattern in patterns:
            match = re.search(pattern, text, flags=re.IGNORECASE)
            if match:
                code = match.group(1).upper()

                # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∑–ª–µ –∫–æ–¥–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                description = extract_description_near(paragraphs, p)

                found.append({
                    "code": code,
                    "description": description,
                    "url": url
                })

    return found


def extract_description_near(paragraphs, code_paragraph):
    """
    –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–º–æ–∫–æ–¥–∞ —Ä—è–¥–æ–º —Å –∞–±–∑–∞—Ü–µ–º –≥–¥–µ –æ–Ω –Ω–∞–π–¥–µ–Ω.
    –ù–µ –±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤. –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ ¬´—à—É–º–Ω–æ–µ¬ª ‚Äî –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º.
    """
    index = paragraphs.index(code_paragraph)

    candidates = []

    # –¢–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü
    text = code_paragraph.get_text(" ", strip=True)
    cleaned = clean_description_text(text)
    if cleaned:
        candidates.append(cleaned)

    # –ê–±–∑–∞—Ü –≤—ã—à–µ
    if index > 0:
        above = clean_description_text(paragraphs[index - 1].get_text(" ", strip=True))
        if above:
            candidates.append(above)

    # –ê–±–∑–∞—Ü –Ω–∏–∂–µ
    if index < len(paragraphs) - 1:
        below = clean_description_text(paragraphs[index + 1].get_text(" ", strip=True))
        if below:
            candidates.append(below)

    # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π
    return candidates[0] if candidates else None


def clean_description_text(text: str):
    """
    –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è:
    - –Ω–µ –¥–ª–∏–Ω–Ω–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
    - –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ –ø–æ—Ö–æ–∂–µ–µ –Ω–∞ –≤—ã–≥–æ–¥—É/–±–æ–Ω—É—Å
    """
    if not text or len(text) > 200:
        return None

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä—è–¥–æ–º —Å –∫–æ–¥–æ–º
    keywords = ["Free", "%", "off", "Boost", "Bundle", "Coupon", "XP"]

    if any(key.lower() in text.lower() for key in keywords):
        return text

    return None

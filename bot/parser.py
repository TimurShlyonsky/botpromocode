import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import re

BASE_URL = "https://www.lotro.com"


def get_month_news(year: int, month: int) -> list[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–µ—Å—è—Ü–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
    –∏–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ JSON (newsEntries), —Ç.–∫. –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥–≥—Ä—É–∂–∞–µ—Ç—Å—è JS.
    """
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"üîé Fetching archive: {url}")

    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load archive page: {e}")
        return []

    # –ò—â–µ–º JSON –º–∞—Å—Å–∏–≤ newsEntries
    match = re.search(r'"newsEntries":\s*(\[[^\]]*\])', response.text, flags=re.DOTALL)
    if not match:
        print("‚ö†Ô∏è No newsEntries found on page.")
        return []

    try:
        entries = json.loads(match.group(1))
    except Exception as e:
        print(f"‚ùå Failed to parse JSON: {e}")
        return []

    links = set()

    for entry in entries:
        href = entry.get("url")
        if href:
            full_url = urljoin(BASE_URL, href)
            links.add(full_url)

    print(f"üîó Found {len(links)} news links for this month")
    return sorted(list(links))


def extract_promo_from_news(url: str):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∏ –∏—â–µ—Ç –ø—Ä–æ–º–æ–∫–æ–¥—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤: {"code", "description", "url"}
    """
    try:
        res = requests.get(url, timeout=20)
        res.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load news page: {url} | {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    # –°–±–æ—Ä –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤, –≥–¥–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–¥—ã
    paragraphs = soup.find_all(["p", "div", "span", "li"])

    patterns = [
        r"Coupon Code[: ]+([A-Z0-9]+)",
        r"Use Code[: ]+([A-Z0-9]+)",
        r"Use coupon code[: ]+([A-Z0-9]+)",
        r"Code[: ]+([A-Z0-9]+)",
        r"Coupon[: ]+([A-Z0-9]+)"
    ]

    found = []

    for i, p in enumerate(paragraphs):
        text = " ".join(p.get_text(" ", strip=True).split())
        if not text:
            continue

        for pattern in patterns:
            match = re.search(pattern, text, flags=re.IGNORECASE)
            if match:
                code = match.group(1).upper()
                description = extract_description_near(paragraphs, i)

                found.append({
                    "code": code,
                    "description": description,
                    "url": url
                })

    return found


def extract_description_near(paragraphs, index: int):
    """
    –ò—â–µ–º —Ç–µ–∫—Å—Ç-–æ–ø–∏—Å–∞–Ω–∏–µ —Ä—è–¥–æ–º —Å –∞–±–∑–∞—Ü–µ–º –∫–æ–¥–∞.
    """
    candidates = []

    def add_candidate(i):
        if 0 <= i < len(paragraphs):
            t = clean_description_text(paragraphs[i].get_text(" ", strip=True))
            if t:
                candidates.append(t)

    add_candidate(index)
    add_candidate(index - 1)
    add_candidate(index + 1)

    return candidates[0] if candidates else None


def clean_description_text(text: str):
    """
    –£–¥–∞–ª—è–µ–º —à—É–º, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–ª–µ–∑–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ.
    """
    if not text or len(text) > 200:
        return None

    keywords = ["Free", "%", "off", "Boost", "Bundle", "XP", "Tome", "Item"]

    if any(key.lower() in text.lower() for key in keywords):
        return text

    return None


if __name__ == "__main__":
    # –¢–µ—Å—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ ‚Äì –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –¥–∞—Ç—É
    links = get_month_news(2025, 12)
    print("News links:", links)
    for link in links:
        promos = extract_promo_from_news(link)
        if promos:
            print("FOUND:", promos)

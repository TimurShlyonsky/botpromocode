import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import re

BASE_URL = "https://www.lotro.com"


def get_month_news(year: int, month: int) -> list[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–µ—Å—è—Ü–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
    –∏–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ JSON (window.SSG.archive.articles –∏–ª–∏ newsEntries).
    """
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"üîé Fetching archive: {url}")

    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load archive page: {e}")
        return []

    text = response.text

    # 1Ô∏è‚É£ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–ø–æ—Å–æ–± ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã LOTRO
    match = re.search(r"window\.SSG\.archive\.articles\s*=\s*(\[[\s\S]*?\]);",
                      text)
    entries = None

    if match:
        try:
            entries = json.loads(match.group(1))
            print("‚úî Found window.SSG.archive.articles JSON")
        except Exception as e:
            print(f"‚ùå Failed to parse archive.articles JSON: {e}")

    # 2Ô∏è‚É£ fallback ‚Äî —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
    if not entries:
        match2 = re.search(r'"newsEntries":\s*(\[[\s\S]*?\])', text)
        if match2:
            try:
                entries = json.loads(match2.group(1))
                print("‚úî Found newsEntries JSON")
            except Exception as e:
                print(f"‚ùå Failed to parse newsEntries JSON: {e}")

    if not entries:
        print("‚ö†Ô∏è No JSON entries found on archive page.")
        return []

    links = set()
    for entry in entries:
        href = entry.get("url") or entry.get("pageName") or entry.get("link")
        if not href:
            continue

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "/"
        if not href.startswith("/"):
            href = "/" + href

        # –ï—Å–ª–∏ –Ω–µ—Ç "/news/", –¥–æ–±–∞–≤–ª—è–µ–º
        if "/news/" not in href:
            href = "/news" + href

        full_url = urljoin(BASE_URL, href)
        links.add(full_url)

    print(f"üîó Found {len(links)} news links for this month")
    return sorted(list(links))


def extract_promo_from_news(url: str):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∏ –∏—â–µ—Ç –ø—Ä–æ–º–æ–∫–æ–¥—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤: {"code", "description", "url"}
    """
    try:
        res = requests.get(url, timeout=20)
        res.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load news page: {url} | {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    paragraphs = soup.find_all(["p", "div", "span", "li"])

    patterns = [
        r"Coupon Code[: ]+([A-Z0-9]+)",
        r"Use Code[: ]+([A-Z0-9]+)",
        r"Use coupon code[: ]+([A-Z0-9]+)",
        r"Coupon[: ]+([A-Z0-9]+)",
        r"Code[: ]+([A-Z0-9]+)",
    ]

    found = []

    for i, p in enumerate(paragraphs):
        text = " ".join(p.get_text(" ", strip=True).split())
        if not text:
            continue

        for pattern in patterns:
            match = re.search(pattern, text, flags=re.IGNORECASE)
            if match:
                code = match.group(1).upper()
                print(f"‚ú® Found code {code} in: {url}")

                description = extract_description_near(paragraphs, i)

                found.append({
                    "code": code,
                    "description": description,
                    "url": url
                })

    return found


def extract_description_near(paragraphs, index: int):
    """–ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Ä—è–¥–æ–º —Å –∞–±–∑–∞—Ü–µ–º, –≥–¥–µ –Ω–∞–π–¥–µ–Ω –∫–æ–¥."""
    candidates = []

    def add(i):
        if 0 <= i < len(paragraphs):
            t = clean_description_text(paragraphs[i].get_text(" ", strip=True))
            if t:
                candidates.append(t)

    add(index)      # —Ç–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü
    add(index - 1)  # –≤—ã—à–µ
    add(index + 1)  # –Ω–∏–∂–µ

    return candidates[0] if candidates else None


def clean_description_text(text: str):
    """–û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–ª–µ–∑–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ."""
    if not text or len(text) > 200:
        return None

    keywords = ["Free", "%", "off", "Boost", "Bundle", "XP", "Tome", "Item", "Crate"]

    if any(k.lower() in text.lower() for k in keywords):
        return text
    return None


if __name__ == "__main__":
    links = get_month_news(2025, 12)
    print("News links:", links)
    for link in links:
        promos = extract_promo_from_news(link)
        if promos:
            print("FOUND:", promos)

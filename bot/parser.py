import re
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE_URL = "https://www.lotro.com"
NEWS_PREFIX = f"{BASE_URL}/news/"

HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept-Language": "en-US,en;q=0.9",
}

# –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ "–∫–æ–¥—ã", –Ω–æ –∏–º–∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è
BAD_CODES = {"UPDATE", "REMINDER", "THROUGH", "CODE", "FOR", "IS", "OF", "IN", "HAS", "FREE", "WILL"}


def fetch_archive_news(year: int, month: int) -> list[str]:
    """
    –î–æ—Å—Ç–∞—ë–º JSON-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–æ—Å—Ç—è—Ö –º–µ—Å—è—Ü–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã—Ö URL.
    """
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"üìÇ –ê—Ä—Ö–∏–≤: {url}")

    try:
        res = requests.get(url, headers=HEADERS, timeout=20)
        res.raise_for_status()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—Ä—Ö–∏–≤–∞: {e}")
        return []

    # –ò—â–µ–º JSON —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –Ω–∞ –∞—Ä—Ö–∏–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    match = re.search(r"window\.SSG\.archive\.articles\s*=\s*(\[[^\]]+\])", res.text)
    if not match:
        print("‚ö†Ô∏è JSON —Å–æ —Å–ø–∏—Å–∫–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return []

    try:
        articles = json.loads(match.group(1))
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        return []

    urls = []
    for a in articles:
        page = a.get("pageName")
        if page:
            urls.append(NEWS_PREFIX + page)

    print(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(urls)}")
    return urls


def extract_promo_from_news(url: str) -> list[dict]:
    """
    –°–∫–∞—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏ –∏—â–µ–º –ø—Ä–æ–º–æ–∫–æ–¥—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤:
    {
      "code": ...,
      "title": ...,
      "url": ...,
      "description": ...
    }
    """
    try:
        res = requests.get(url, headers=HEADERS, timeout=20)
        res.raise_for_status()
    except Exception as e:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {url} ‚Äî –æ—à–∏–±–∫–∞ {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")
    title = soup.title.get_text(strip=True) if soup.title else "LOTRO News"

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
    body = soup.select_one(".article-body")
    if not body:
        return []

    text = body.get_text(" ", strip=True)
    text_up = text.upper()

    # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞: COUPON CODE: ANDIRUN
    matches = re.findall(
        r"(?:COUPON CODE|USE CODE|USE COUPON CODE|CODE|COUPON)[:\s]+([A-Z0-9]+)",
        text_up
    )

    found = []
    for code in matches:
        code = code.strip().upper()

        if len(code) < 5:
            continue
        if code in BAD_CODES:
            continue

        found.append({
            "code": code,
            "title": title,
            "url": url,
            "description": extract_near_description(text, code),
        })

    if found:
        print(f"‚ú® –ù–∞–π–¥–µ–Ω–æ –≤ {url}: {[f['code'] for f in found]}")
    return found


def extract_near_description(full_text: str, code: str):
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∏–π –ø–æ–∏—Å–∫ –æ–ø–∏—Å–∞–Ω–∏—è ‚Äî –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ –∫–æ–¥.
    –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É, —á—Ç–æ–±—ã –Ω–µ —Ç–∞—â–∏—Ç—å –≤—Å—é —Å—Ç–∞—Ç—å—é.
    """
    sentences = re.split(r"[.!?]", full_text)
    for s in sentences:
        if code in s.upper():
            s = s.strip()
            if 10 <= len(s) <= 200:
                return s
    return None

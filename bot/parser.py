import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE_URL = "https://www.lotro.com"


def get_month_news(year: int, month: int) -> list[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—Ä—Ö–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏.
    """
    url = f"{BASE_URL}/archive/{year}/{month:02d}"
    print(f"üîé Fetching archive: {url}")

    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to load archive page: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")

    # –í –∞—Ä—Ö–∏–≤–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–¥–≥—Ä—É–∂–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ JS
    # –ù–æ —Å—Å—ã–ª–∫–∏ –µ—Å—Ç—å –≤ HTML –≤–Ω—É—Ç—Ä–∏ —Ç–µ–º–ø–ª–µ–π—Ç–∞ ‚Üí –∏—â–µ–º –≤—Å–µ <a href>
    links = set()
    for a in soup.find_all("a", href=True):
        href = a["href"]
        # –ò–Ω—Ç–µ—Ä–µ—Å—É—é—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏
        if href.startswith("/news/") or href.startswith("/update-notes/") or href.startswith("/guides/"):
            full_url = urljoin(BASE_URL, href)
            links.add(full_url)

    return sorted(list(links))


if __name__ == "__main__":
    # –¢–µ—Å—Ç: –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –¥–µ–∫–∞–±—Ä—å 2025
    urls = get_month_news(2025, 12)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Ç–∞—Ç–µ–π:")
    for u in urls:
        print(" -", u)
